{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Convolutional Neural Network\n",
    "***Honors Project by Peyton Warren***\n",
    "\n",
    "This neural net uses the MNIST dataset with the listed layers:\n",
    "1. Convolutional layer\n",
    "2. Max Pooling layer\n",
    "3. Fully Connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data setup\n",
    "# Download mnist dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# creating the image dimension variables and the number of channels\n",
    "img_x, img_y = x_train.shape[1], x_train.shape[2]\n",
    "channels = 1\n",
    "\n",
    "plt.imshow(x_train[59999], cmap=plt.cm.binary)\n",
    "\n",
    "# Reshape into 4D tensor with tensorflow reshape function\n",
    "# Putting it into the format \"channels_last\" data format (batch, cols, rows, channels)\n",
    "x_train = x_train.reshape(x_train.shape[0], img_x, img_y, channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_x, img_y, channels)\n",
    "\n",
    "# change data type to floating point numbers\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize the data, x_train & x_test now between 0 and 1\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# print(type(x_train))    # x_train is a numpy.ndarray object\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for training the model later on\n",
    "batch_size = 128    # 128 items in the training data are being used\n",
    "num_classes = 10    # Number of classifications\n",
    "epochs = 10         # performing 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Adds layers\n",
    "\n",
    "# Convolutional Layer\n",
    "model.add(Conv2D(32,\n",
    "                 kernel_size = (5, 5), strides = (1, 1), # (5, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(img_x, img_y, channels)))\n",
    "\n",
    "# Max Pooling Layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Fully Connected Layer\n",
    "model.add(Dense(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer: Stochastic Gradient Descent\n",
    "sgd = SGD(lr=0.0001)\n",
    "\n",
    "# Compile follows setting up the neural network\n",
    "model.compile(optimizer=sgd,\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_14 to have 4 dimensions, but got array with shape (60000, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-543a71198e90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0;31m# verbose 1 means it has a progress bar for every epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training_utils.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    125\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_14 to have 4 dimensions, but got array with shape (60000, 1)"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,      # inputing the training x and y\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,             # verbose 1 means it has a progress bar for every epoch\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
